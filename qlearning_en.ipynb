{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RipY6NNROJvA"
      },
      "source": [
        "# Q-learning for solving a simple maze"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvGfA7QdOLbX"
      },
      "source": [
        "## Program of Q-learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MrUxNKLUKjSJ",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# coding: utf-8\n",
        "\n",
        "###################################################################\n",
        "# Q-learning for solving a maze.\n",
        "# References:\n",
        "#   小川：作りながら学ぶ深層強化学習，マイナビ出版 (2018).\n",
        "#   https://github.com/YutaroOgawa/Deep-Reinforcement-Learning-Book\n",
        "###################################################################\n",
        "\n",
        "# Pckages\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# This function transforms the policy parameter theta to a policy pi\n",
        "def simple_convert_into_pi_from_theta(theta):\n",
        "    [m, n] = theta.shape  # Obtains the matrix size of theta.\n",
        "    pi = np.zeros((m, n))\n",
        "    for i in range(0, m):\n",
        "        pi[i, :] = theta[i, :] / np.nansum(theta[i, :])\n",
        "    pi = np.nan_to_num(pi)  # Converts nan to 0.\n",
        "    return pi\n",
        "\n",
        "# This function chooses an action with the epsilon-greedy method.\n",
        "def get_action(s, Q, epsilon, pi_0):\n",
        "    direction = [\"up\", \"right\", \"down\", \"left\"]\n",
        "    if np.random.rand() < epsilon:\n",
        "        next_direction = np.random.choice(direction, p=pi_0[s, :]) # Chooses an action randomly.\n",
        "    else:\n",
        "        next_direction = direction[np.nanargmax(Q[s, :])] # Selects the actoin whose Q is the leargest.\n",
        "    # 行動をindexに\n",
        "    if next_direction == \"up\":\n",
        "        action = 0\n",
        "    elif next_direction == \"right\":\n",
        "        action = 1\n",
        "    elif next_direction == \"down\":\n",
        "        action = 2\n",
        "    elif next_direction == \"left\":\n",
        "        action = 3\n",
        "    return action\n",
        "\n",
        "# This function returns the next state.\n",
        "def get_s_next(s, a, Q, epsilon, pi_0):\n",
        "    direction = [\"up\", \"right\", \"down\", \"left\"]\n",
        "    next_direction = direction[a]\n",
        "    if next_direction == \"up\":\n",
        "        s_next = s - 3  # If the agent moves up, the state number decreases by 3.\n",
        "    elif next_direction == \"right\":\n",
        "        s_next = s + 1  # If the agent moves right, the state number increases by 1.\n",
        "    elif next_direction == \"down\":\n",
        "        s_next = s + 3  # If the agent moves down, the state number increases by 3.\n",
        "    elif next_direction == \"left\":\n",
        "        s_next = s - 1  # if the agent moves left, the state number decreases by 1.\n",
        "    return s_next\n",
        "\n",
        "# This function makes the agent run until the agent reaches the goal and returns the history of states and actions and Q.\n",
        "def goal_maze_ret_s_a_Q(Q, epsilon, eta, gamma, pi):\n",
        "    s = 0  # The initial state\n",
        "    a = a_next = get_action(s, Q, epsilon, pi)  # An initial action\n",
        "    s_a_history = [[0, np.nan]]  # The history in which states and actions are recorded.\n",
        "    while (1):  # Repeates until the agent reaches the goal.\n",
        "        a = a_next # The current action\n",
        "        s_a_history[-1][1] = a # Assigns the action of the current state.\n",
        "        s_next = get_s_next(s, a, Q, epsilon, pi) # The next state\n",
        "        s_a_history.append([s_next, np.nan]) # Assign the next state. Let the action nan because it is unknown.\n",
        "        if s_next == 8: # The case where the agent reaches the goal.\n",
        "            r = 1  # The reward\n",
        "            a_next = np.nan\n",
        "        else:\n",
        "            r = 0\n",
        "            a_next = get_action(s_next, Q, epsilon, pi) # Chosses the next action.\n",
        "        Q = Q_learning(s, a, r, s_next, Q, eta, gamma) # Updates Q.\n",
        "        if s_next == 8:  # if the agent reaches the goal, terminate the loop.\n",
        "            break\n",
        "        else:\n",
        "            s = s_next\n",
        "    return [s_a_history, Q]\n",
        "\n",
        "# This function updates Q by Q-Learning\n",
        "def Q_learning(s, a, r, s_next, Q, eta, gamma):\n",
        "    if s_next == 8:  # The case where the agent reaches the goal.\n",
        "        Q[s, a] = Q[s, a] + eta * (r - Q[s, a])\n",
        "    else:\n",
        "        Q[s, a] = Q[s, a] + eta * (r + gamma * np.nanmax(Q[s_next,: ]) - Q[s, a])\n",
        "    return Q\n",
        "\n",
        "# Main\n",
        "eta = 0.1  # Learning rate\n",
        "gamma = 0.9  # Discount factor\n",
        "epsilon = 0.5  # Initial epsilon\n",
        "max_episode = 14\n",
        "# The initial policy parameter, theta_0.\n",
        "# The row represetns state numbers:  0-7. The column represents actions: up, right, down and left.\n",
        "theta_0 = np.array([[np.nan, 1, 1, np.nan],  # s0\n",
        "                    [np.nan, 1, np.nan, 1],  # s1\n",
        "                    [np.nan, np.nan, 1, 1],  # s2\n",
        "                    [1, 1, 1, np.nan],  # s3\n",
        "                    [np.nan, np.nan, 1, 1],  # s4\n",
        "                    [1, np.nan, np.nan, np.nan],  # s5\n",
        "                    [1, np.nan, np.nan, np.nan],  # s6\n",
        "                    [1, 1, np.nan, np.nan],  # s7\n",
        "                    ])\n",
        "pi_0 = simple_convert_into_pi_from_theta(theta_0) # Obtains a random policy, pi_0.\n",
        "[a, b] = theta_0.shape  # Obtains the matrix size of theta_0.\n",
        "Q_0 = np.random.rand(a, b) * theta_0 # Makes the Q-values corresponding to the wall nan.\n",
        "Q = Q_0.copy()\n",
        "episode = 1\n",
        "steps = []\n",
        "while True:\n",
        "    epsilon = epsilon / 2 # Decreases epsilon gradually.\n",
        "    [s_a_history, Q] = goal_maze_ret_s_a_Q(Q, epsilon, eta, gamma, pi_0)\n",
        "    steps.append(len(s_a_history) - 1)\n",
        "    episode = episode + 1\n",
        "    if episode > max_episode:\n",
        "        break\n",
        "print(\"\\n<<Initial Q>>\")\n",
        "print(Q_0)\n",
        "print(\"\\n<<Final Q>>\")\n",
        "print(Q)\n",
        "\n",
        "# plot steps\n",
        "plt.plot(steps)\n",
        "print(f\"\\nThe number of steps of the first episode: {steps[0]}\")\n",
        "print(f\"The number of steps of the last episode: {steps[len(steps) - 1]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gp_BSLNdOUsh"
      },
      "source": [
        "## Program for animation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Gcj0ja3KjSM",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# This program visualizes how the agent moved in the maze.\n",
        "# Reference: http://louistiao.me/posts/notebooks/embedding-matplotlib-animations-in-jupyter-notebooks/\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "get_ipython().run_line_magic('matplotlib', 'inline')\n",
        "from matplotlib import animation\n",
        "from IPython.display import HTML\n",
        "\n",
        "def init():\n",
        "    line.set_data([], [])\n",
        "    return (line,)\n",
        "\n",
        "\n",
        "def animate(i):\n",
        "    state = s_a_history[i][0]\n",
        "    x = (state % 3) + 0.5\n",
        "    y = 2.5 - int(state / 3)\n",
        "    line.set_data([x], [y]) # Pass x and y as lists\n",
        "    return (line,)\n",
        "\n",
        "# Main\n",
        "fig = plt.figure(figsize=(5, 5))\n",
        "ax = plt.gca()\n",
        "plt.plot([1, 1], [0, 1], color='red', linewidth=2)\n",
        "plt.plot([1, 2], [2, 2], color='red', linewidth=2)\n",
        "plt.plot([2, 2], [2, 1], color='red', linewidth=2)\n",
        "plt.plot([2, 3], [1, 1], color='red', linewidth=2)\n",
        "plt.text(0.5, 2.5, 'S0', size=14, ha='center')\n",
        "plt.text(1.5, 2.5, 'S1', size=14, ha='center')\n",
        "plt.text(2.5, 2.5, 'S2', size=14, ha='center')\n",
        "plt.text(0.5, 1.5, 'S3', size=14, ha='center')\n",
        "plt.text(1.5, 1.5, 'S4', size=14, ha='center')\n",
        "plt.text(2.5, 1.5, 'S5', size=14, ha='center')\n",
        "plt.text(0.5, 0.5, 'S6', size=14, ha='center')\n",
        "plt.text(1.5, 0.5, 'S7', size=14, ha='center')\n",
        "plt.text(2.5, 0.5, 'S8', size=14, ha='center')\n",
        "plt.text(0.5, 2.3, 'START', ha='center')\n",
        "plt.text(2.5, 0.3, 'GOAL', ha='center')\n",
        "ax.set_xlim(0, 3)\n",
        "ax.set_ylim(0, 3)\n",
        "plt.tick_params(axis='both', which='both', bottom=False, top=False,\n",
        "                labelbottom=False, right=False, left=False, labelleft=False)\n",
        "line, = ax.plot([0.5], [2.5], marker=\"o\", color='g', markersize=60)\n",
        "anim = animation.FuncAnimation(fig, animate, init_func=init, frames=len(s_a_history), interval=200, repeat=False)\n",
        "HTML(anim.to_jshtml())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hVJLa9pKjSN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QvJYvAzFKjSN"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}